{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n",
    "    torch.save(state, filename)\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, 'model_best.pth.tar')\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, epoch):\n",
    "    \"\"\"Sets the learning rate to the initial LR decayed by 10 every 30 epochs\"\"\"\n",
    "    lr = args.lr * (0.1 ** (epoch // 30))\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the accuracy over the k top predictions for the specified values of k\"\"\"\n",
    "    with torch.no_grad():\n",
    "        maxk = max(topk)\n",
    "        batch_size = target.size(0)\n",
    "\n",
    "        _, pred = output.topk(maxk, 1, True, True)\n",
    "        pred = pred.t()\n",
    "        correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "        res = []\n",
    "        for k in topk:\n",
    "            correct_k = correct[:k].view(-1).float().sum(0, keepdim=True)\n",
    "            res.append(correct_k.mul_(100.0 / batch_size))\n",
    "        return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "\n",
    "class Baseline(nn.Module):\n",
    "\n",
    "    def __init__(self, in_ch, out_ch):\n",
    "        \n",
    "        super(Baseline, self).__init__()\n",
    "        '''\n",
    "        torch.nn.Conv3d: input(N,C,D,H,W)\n",
    "                            output(N,C,Dout,Hout,Wout) \n",
    "        torch.nn.AvgPool3d: input(N,C,D,H,W)\n",
    "                            output(N,C,Dout,Hout,Wout)     \n",
    "        '''\n",
    "        '''\n",
    "        nn.Conv3d(in_channels, out_channels, kernel_size)\n",
    "        nn.AvgPool3d()\n",
    "        '''\n",
    "        self.draft_model = nn.Sequential(\n",
    "            nn.Conv3d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "#             nn.AvgPool3d(3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv3d(in_ch, out_ch, 3, stride=1, padding=1),\n",
    "            nn.BatchNorm3d(out_ch),\n",
    "#             nn.AvgPool3d(3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward(self, cube):\n",
    "        cube = self.draft_model(cube)\n",
    "        return cube\n",
    "    \n",
    "class SimpleUnet(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        \n",
    "        super(SimpleUnet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.conv1=nn.Conv3d(self.in_channels, 16, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.conv2=nn.Conv3d(16, 16, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.pool1=nn.AvgPool3d(2)\n",
    "        self.conv3=nn.Conv3d(16, 32, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.pool2=nn.AvgPool3d(2)\n",
    "        self.conv4=nn.Conv3d(32, 64, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "#         self.upsamp1= nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#         self.upconv1=nn.Conv3d(64, 64, kernel_size=3, stride=1, padding=1, bias=False)        \n",
    "#         self.conv5=nn.Conv3d(64, 32, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "#         self.upsamp2= nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#         self.upconv2=nn.Conv3d(32, 32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.conv6=nn.Conv3d(32, 16, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "#         self.upsamp3= nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#         self.upconv3=nn.Conv3d(16, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.conv7=nn.Conv3d(16, 8, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "#         self.upsamp4= nn.Upsample(scale_factor=2, mode='nearest')\n",
    "#         self.upconv4=nn.Conv3d(8, 8, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "#         self.conv8=nn.Conv3d(8, 4, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "#         self.conv9=nn.Conv3d(4, 1, kernel_size=3, stride=1, padding=0, bias=True)\n",
    "        self.relu=nn.ReLU()\n",
    "#         self.fc1=nn.Linear(43904,32768)\n",
    "#         self.fc2=nn.Linear(6912, 32768)\n",
    "        self.fc3=nn.Linear(4096,32768*2)\n",
    "#         self.Simple_Unet = nn.Sequential(\n",
    "#             self.conv_layer(self.in_channels, 16),\n",
    "#             self.conv_layer(16, 16),\n",
    "#             nn.AvgPool3d(2),\n",
    "#             self.conv_layer(16,32),\n",
    "#             nn.AvgPool3d(2),\n",
    "#             self.conv_layer(32,64),\n",
    "#             self.up_conv_layer(64, 64, 3),\n",
    "#             self.conv_layer(64, 32),\n",
    "#             self.up_conv_layer(32, 32, 3),\n",
    "#             self.conv_layer(32, 16),\n",
    "#             self.up_conv_layer(16, 16, 3),\n",
    "#             self.conv_layer(16, 8),\n",
    "#             self.up_conv_layer(8, 8, 3),\n",
    "#             self.conv_layer(8, 4),\n",
    "#             self.conv_layer(4, 1)\n",
    "#         )\n",
    "    \n",
    "#     def conv_layer(self, inputs, in_channels, out_channels, kernel_size=3, stride=1, padding=0, bias=True):\n",
    "# #         layers = nn.Sequential(\n",
    "#         nn.Conv3d(in_channels, out_channels, kernel_size, stride=stride, padding=padding, bias=bias)\n",
    "#         # nn.BatchNorm3d(out_channels),\n",
    "#         nn.ReLU()\n",
    "# #         print(layers.size())\n",
    "#         return layers\n",
    "    \n",
    "#     def up_conv_layer(self, in_channels, out_channels, kernel_size, stride=1, padding=0, output_padding=0, bias=True):\n",
    "#         layers = nn.Sequential(\n",
    "#             nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "#             # should be feat_in*2 or feat_in\n",
    "#             nn.Conv3d(in_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "#             # nn.BatchNorm3d(out_channels),\n",
    "#             nn.ReLU())\n",
    "#         return layers\n",
    "    \n",
    "    \n",
    "    def forward(self, cube):\n",
    "        b_size=cube.size()[0]\n",
    "        cube = self.conv1(cube)\n",
    "        cube = self.relu(cube)\n",
    "#         print(cube.size())\n",
    "        cube = self.conv2(cube)\n",
    "        cube = self.relu(cube)\n",
    "#         print(cube.size())\n",
    "        cube = self.pool1(cube)\n",
    "#         print(cube.size())\n",
    "#         linear1=self.fc1(cube.view(b_size,-1))\n",
    "        cube = self.conv3(cube)\n",
    "        cube = self.relu(cube)\n",
    "#         print(cube.size())\n",
    "        cube = self.pool2(cube)\n",
    "#         print(cube.size())\n",
    "#         linear2=self.fc2(cube.view(b_size,-1))\n",
    "        cube = self.conv4(cube)\n",
    "        cube = self.relu(cube)\n",
    "#         print(cube.size())\n",
    "        linear=self.fc3(cube.view(b_size,-1))\n",
    "#         linear=linear1+linear2+linear3\n",
    "        cube=linear.reshape([b_size,3,32,32,32])\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.upsamp1(cube)\n",
    "#         cube = self.upconv1(cube)\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.conv5(cube)\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.upsamp2(cube)\n",
    "#         cube = self.upconv2(cube)\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.conv6(cube)\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.upsamp3(cube)\n",
    "#         cube = self.upconv3(cube)\n",
    "#         cube = self.relu(cube)        \n",
    "#         cube = self.conv7(cube)\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.upsamp4(cube)\n",
    "#         cube = self.upconv4(cube)\n",
    "#         cube = self.relu(cube) \n",
    "#         cube = self.conv8(cube)\n",
    "#         cube = self.relu(cube)\n",
    "#         cube = self.conv9(cube)\n",
    "#         cube = self.relu(cube)\n",
    "        return cube"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class args:\n",
    "    lr = 0.001\n",
    "    momentum = 0.9\n",
    "    weight_decay = 0\n",
    "    start_epoch = 0\n",
    "    epochs = 50\n",
    "    print_freq = 400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_class(num):\n",
    "    if num==0:\n",
    "        return 0\n",
    "    elif num>0:\n",
    "        return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "import numpy as np\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, lists):\n",
    "        'Initialization'\n",
    "        self.IDs = lists\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.IDs[index]\n",
    "        d_box=np.load('/scratch/xz2139/cosmo_dark/arrays/'+str(ID[0])+'_'+str(ID[1])+'_'+str(ID[2])+'.npy')\n",
    "        f_box=np.load('/scratch/xz2139/cosmo_full/arrays/'+str(ID[0])+'_'+str(ID[1])+'_'+str(ID[2])+'.npy')\n",
    "        convert= np.vectorize(convert_class) #Convert python function to vector function\n",
    "        f_box=convert(f_box)\n",
    "        return d_box,f_box\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "arguments: <class '__main__.args'>\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "# import torch.utils.data.distributed\n",
    "# import torchvision.transforms as transforms\n",
    "# import torchvision.datasets as datasets\n",
    "# import torchvision.models as models\n",
    "\n",
    "from torch.utils import data\n",
    "import random\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "import argparse\n",
    "\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser(description=\"main.py\")\n",
    "    parser.add_argument('--mini', type=int, default=0,\n",
    "                        help='whether to use mini dataset.')\n",
    "    parser.add_argument('--medium', type=int, default=0,\n",
    "                        help='whether to use medium dataset.')\n",
    "    \n",
    "    return parser.parse_args()\n",
    "\n",
    "\n",
    "print(\"arguments: %s\" %(args))\n",
    "\n",
    "def initial_loss(train_loader, val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    train_losses = AverageMeter()\n",
    "    val_losses = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (input, target) in enumerate(train_loader):\n",
    "            # add a dimension, from (1, 32, 32, 32) to (1,1,32,32,32)\n",
    "            input = input.unsqueeze(dim = 1).to(device).float()\n",
    "#             target = target.unsqueeze(dim = 1).to(device).float()\n",
    "            target = target.to(device).long()\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            # measure accuracy and record loss\n",
    "            train_losses.update(loss.item(), input.size(0))\n",
    "\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            # add a dimension, from (1, 32, 32, 32) to (1,1,32,32,32)\n",
    "            input = input.unsqueeze(dim = 1).to(device).float()\n",
    "#             target = target.unsqueeze(dim = 1).to(device).float()\n",
    "            target = target.to(device).long()\n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            \n",
    "            loss = criterion(output, target)\n",
    "            # measure accuracy and record loss\n",
    "            val_losses.update(loss.item(), input.size(0))\n",
    "\n",
    "\n",
    "        \n",
    "        print('Training initial Loss {train_loss.avg:.4f}\\t'\n",
    "        \t'Validation initial Loss {val_loss.avg:.4f}\\t'.format(train_loss=train_losses, val_loss = val_losses))\n",
    "\n",
    "\n",
    "\n",
    "def train(train_loader, model, criterion, optimizer, epoch):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    data_time = AverageMeter()\n",
    "\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        # measure data loading time\n",
    "        data_time.update(time.time() - end)\n",
    "\n",
    "\n",
    "\n",
    "        # add a dimension, from (1, 32, 32, 32) to (1,1,32,32,32)\n",
    "        input = input.unsqueeze(dim = 1).to(device).float()\n",
    "#         target = target.unsqueeze(dim = 1).to(device).long()\n",
    "        target = target.to(device).long()\n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        #print(torch.nonzero(target).size())\n",
    "        loss = criterion(output, target)\n",
    "        # measure accuracy and record loss\n",
    "        losses.update(loss.item(), input.size(0))\n",
    "\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % args.print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'.format(\n",
    "                   epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                   data_time=data_time, loss=losses))\n",
    "\n",
    "    print('Epoch Train: Loss {loss.avg:.4f}\\t'.format(loss=losses))\n",
    "\n",
    "def validate(val_loader, model, criterion):\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    # switch to evaluate mode  \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(val_loader):\n",
    "            input = input.unsqueeze(dim = 1).to(device).float()\n",
    "#             target = target.unsqueeze(dim = 1).to(device).float()\n",
    "            target = target.to(device).long()\n",
    "            \n",
    "            \n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            outputs = F.softmax(output, dim=1)\n",
    "            predicted = outputs.max(1, keepdim=True)[1]\n",
    "            total += np.prod(target.shape)\n",
    "            correct += predicted.eq(target.view_as(predicted)).sum().item()\n",
    "#             output=0*output\n",
    "            loss = criterion(output, target)\n",
    "            # measure accuracy and record loss\n",
    "            losses.update(loss.item(), input.size(0))\n",
    "\n",
    "            # measure elapsed time\n",
    "            batch_time.update(time.time() - end)\n",
    "            end = time.time()\n",
    "\n",
    "            \n",
    "    print('Epoch Test: Loss {loss.avg:.4f} Accuracy {ac:.4f}\\t'.format(loss=losses,ac=correct/total*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-22adf840cba1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(validation_generator):\n",
    "        input = input.unsqueeze(dim = 1).to(device).float()\n",
    "        target = target.to(device).long()            \n",
    "        # compute output\n",
    "        output = model(input)\n",
    "        loss = criterion(output, target)\n",
    "        break"
    "total=0\n",
    "nonzero=0\n",
    "for i, (input, target) in enumerate(validation_generator):\n",
    "    target = target.to(device).long() \n",
    "    \n",
    "    total+=np.prod(target.shape)\n",
    "    nonzero+=torch.nonzero(target).size(0)\n",
    "print((1-(nonzero/total))*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        end = time.time()\n",
    "        for i, (input, target) in enumerate(validation_generator):\n",
    "            input = input.unsqueeze(dim = 1).to(device).float()\n",
    "            target = target.to(device).long()            \n",
    "            # compute output\n",
    "            output = model(input)\n",
    "            loss = criterion(output, target)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'training_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-1869d84e0ed9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# compute output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'training_generator' is not defined"
     ]
    }
   ],
   "source": [
    "for i, (input, target) in enumerate(training_generator):\n",
    "    input = input.unsqueeze(dim = 1).to(device).float()\n",
    "    target = target.to(device).long()            \n",
    "    # compute output\n",
    "    print(target.size())\n",
    "    output = model(input)\n",
    "    loss = criterion(output, target)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Traceback (most recent call last):\n  File \"/Users/hp/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/hp/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-6-a86cd44094b9>\", line 16, in __getitem__\n    d_box=np.load('/scratch/xz2139/cosmo_dark/arrays/'+str(ID[0])+'_'+str(ID[1])+'_'+str(ID[2])+'.npy')\n  File \"/Users/hp/anaconda/lib/python3.5/site-packages/numpy/lib/npyio.py\", line 372, in load\n    fid = open(file, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/scratch/xz2139/cosmo_dark/arrays/928_608_736.npy'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-9201179dbccd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     50\u001b[0m optimizer = torch.optim.Adam(model.parameters(), args.lr,\n\u001b[1;32m     51\u001b[0m                                 weight_decay=args.weight_decay)\n\u001b[0;32m---> 52\u001b[0;31m \u001b[0minitial_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_generator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-a9b37264cc1f>\u001b[0m in \u001b[0;36minitial_loss\u001b[0;34m(train_loader, val_loader, model, criterion)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0;31m# add a dimension, from (1, 32, 32, 32) to (1,1,32,32,32)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    284\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreorder_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m                 \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_next_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mnext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m__next__\u001b[0m  \u001b[0;31m# Python 2 compatibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_process_next_batch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_put_indices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mExceptionWrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexc_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: Traceback (most recent call last):\n  File \"/Users/hp/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in _worker_loop\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"/Users/hp/anaconda/lib/python3.5/site-packages/torch/utils/data/dataloader.py\", line 57, in <listcomp>\n    samples = collate_fn([dataset[i] for i in batch_indices])\n  File \"<ipython-input-6-a86cd44094b9>\", line 16, in __getitem__\n    d_box=np.load('/scratch/xz2139/cosmo_dark/arrays/'+str(ID[0])+'_'+str(ID[1])+'_'+str(ID[2])+'.npy')\n  File \"/Users/hp/anaconda/lib/python3.5/site-packages/numpy/lib/npyio.py\", line 372, in load\n    fid = open(file, \"rb\")\nFileNotFoundError: [Errno 2] No such file or directory: '/scratch/xz2139/cosmo_dark/arrays/928_608_736.npy'\n"
     ]
    }
   ],

   "source": [
    "# params = parse_args()\n",
    "# mini = params.mini\n",
    "# medium = params.medium\n",
    "mini=False\n",
    "medium=False\n",
    "#index for the cube, each tuple corresponds to a cude\n",
    "#test data\n",
    "if mini:\n",
    "    train_data = [(832, 640, 224),(864, 640, 224)]\n",
    "    val_data = [(832, 640, 224),(864, 640, 224)]\n",
    "    test_data = [(832, 640, 224),(864, 640, 224)]\n",
    "else:\n",
    "    if medium:\n",
    "        data_range = 150\n",
    "    else:\n",
    "        data_range = 1024\n",
    "\n",
    "    pos=list(np.arange(0,data_range,32))\n",
    "    ranges=list(product(pos,repeat=3))\n",
    "    random.shuffle(ranges)\n",
    "    train_data = ranges[:int(np.round(len(ranges)*0.6))]\n",
    "    val_data=ranges[int(np.round(len(ranges)*0.6)):int(np.round(len(ranges)*0.8))]\n",
    "    test_data = ranges[int(np.round(len(ranges)*0.8)):]\n",
    "\n",
    "# #build dataloader\n",
    "params = {'batch_size': 8,\n",
    "      'shuffle': True,\n",
    "      'num_workers':20}\n",
    "\n",
    "training_set, validation_set = Dataset(train_data), Dataset(val_data)\n",
    "testing_set= Dataset(test_data)\n",
    "training_generator = data.DataLoader(training_set, **params)\n",
    "validation_generator = data.DataLoader(validation_set, **params)\n",
    "testing_generator = data.DataLoader(testing_set, **params)\n",
    "\n",
    "# for i, (input, target) in enumerate(training_generator):\n",
    "#     print('input')\n",
    "#     print(input)\n",
    "#     print('target')\n",
    "#     print(target)\n",
    "\n",
    "# #set up device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# #build model\n",
    "dim = 1\n",
    "model = SimpleUnet(dim).to(device)\n",
    "# criterion = nn.MSELoss().to(device) #yueqiu\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), args.lr,\n",
    "                                weight_decay=args.weight_decay)\n",
    "initial_loss(training_generator, validation_generator, model, criterion)\n",
    "\n",
    "for epoch in range(args.start_epoch, args.epochs):\n",
    "    adjust_learning_rate(optimizer, epoch)\n",
    "    train(training_generator, model, criterion, optimizer, epoch)\n",
    "    # evaluate on validation set\n",
    "    validate(validation_generator, model, criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"

  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
